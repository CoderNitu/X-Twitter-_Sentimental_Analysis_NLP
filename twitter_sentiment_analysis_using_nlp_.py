# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis using NLP .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PoyjqmCJ7lCCBCPrnxY91f3NLjXxWrA0

## Dataset Information:

The Objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist and sexist sentiments associatesd with it. So, the task is to classify racist or sexist tweets from other tweets.

Formally, given a training sample of tweets and labels , where label '1' denotes the tweet that is racist and sexist and label '0' denotes the tweet is not racist/sexiat, our objective is to prdict the labels on the test dataset.

For, training the model we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id,it's label and the tweets.

**Impoting Modules/Libraries:**
"""

# Commented out IPython magic to ensure Python compatibility.
# Data manipulation libraries

import pandas as pd
import numpy as np
import re
import string

# Data visualization libraries

import matplotlib.pyplot as plt
import seaborn as sns

# Text processing libraries
import nltk

import warnings
# %matplotlib inline

warnings.filterwarnings('ignore')

"""**Loading the Dataset:**"""

# loading the data from csv file to a pandas dataframe
df = pd.read_csv('/content/id,label,tweet.csv')

# printing the first five rows of the dataframe
df.head()

# number of rows and columns in the dataframe
df.shape

# getting more information about the dataset:
df.info()

# checking for missing values in each column
df.isnull().sum()

"""**Preprocessing the dataset:**

**1. Noise Removal:**
"""

# remove patterns from input text

def remove_pattern(input_txt,pattern):
  r= re.findall(pattern,input_txt)
  for word in r:
    input_txt=re.sub(word,"",input_txt)
  return input_txt

df.head()

# remove twitter handles (i.e @user)

df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'],"@[\w]*")

df.head()

# remove special charcters, numbers and punctuations

df['clean_tweet']=df['clean_tweet'].str.replace("[^a-zA-Z#]"," ")
df.head()

# remove short words

df['clean_tweet']=df['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
df.head()

""" **2. Tokenization:**"""

# Tokenization: (splitting the documents into small meaning words to understand the importance of each word with respect to the sentence)

tokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())
tokenized_tweet.head()

"""**3.Text Normalization:**"""

# (a): Stemming: (Normalize words into its base form or root form by cutting the beginning and ending of the words.)

from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])
tokenized_tweet.head()

# (b): Lemmatization: (Normalize words by linking words with similar meaning word)

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

tokenized_tweet = tokenized_tweet.apply(lambda sentence: [lemmatizer.lemmatize(word) for word in sentence])
tokenized_tweet.head()

# combine the words into a single sentence

for i in range(len(tokenized_tweet)):
  tokenized_tweet[i] = " ".join(tokenized_tweet[i])

df['clean_tweet']=tokenized_tweet
df.head()

"""**Exploratory Data Analysis:**"""

# Visual represention of distribution of positive and negative sentiments using PieChart

fig=plt.figure(figsize=(7,7))
colors= ("yellowgreen","gold","red")
wp = {'linewidth':2, 'edgecolor':"black"}
tags = df['label'].value_counts()
explode=(0.1,0.1)
tags.plot(kind='pie',autopct='%1.1f%%',shadow=True, colors=colors,
          startangle=90, wedgeprops =wp, explode=explode, label="" )
plt.title("Distribution of Sentiments")

! pip install wordcloud

# visualize the frequent words

all_words = " ".join([sentence for sentence in df['clean_tweet']])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# frequent words visualization for +ve

all_words = " ".join([sentence for sentence in df['clean_tweet'][df['label']==0]])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# frequent words visualization for -ve

all_words = " ".join([sentence for sentence in df['clean_tweet'][df['label']==1]])

wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

#extracct the hashtag:

def hashtag_extract(tweets):
  hashtags = []
  # loop tweet in the tweets
  for tweet in tweets:
    ht=re.findall(r"#(\w+)",tweet)
    hashtags.append(ht)
  return hashtags

# extract hashtags from non-racist/sexist tweets

ht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])

# extract hashtags from racist/sexist tweets

ht_negative = hashtag_extract(df['clean_tweet'][df['label']==1])

ht_positive[:5]

# combine it to a single list

ht_positive = sum(ht_positive, [])

ht_negative = sum(ht_negative, [])

ht_positive[:5]

# use dictionary from nltk to preprocessing the data:

freq = nltk.FreqDist(ht_positive)

d = pd.DataFrame({'Hashtag': list(freq.keys()),
                  'count': list(freq.values())})

d.head()

# select top 10 hashtags

d = d.nlargest(columns='count', n=10)
plt.figure(figsize=(15,9))
sns.barplot(data=d, x='Hashtag', y='count')
plt.show()

# use dictionary from nltk to preprocessing the data:

freq = nltk.FreqDist(ht_negative)

d = pd.DataFrame({'Hashtag': list(freq.keys()),
                  'count': list(freq.values())})

d.head()

# select top 10 hashtags

d = d.nlargest(columns='count', n=10)
plt.figure(figsize=(15,9))
sns.barplot(data=d, x='Hashtag', y='count')
plt.show()

"""**Vectorizing Tokens:**"""

# Feature extraction
# CountVectorizer Means Breaking Down A Sentence Or Any Text Into Words By Performing Preprocessing Tasks Like Converting All Words To Lowercase, Thus Removing Special Characters.

# Bag of Word (BOW)ModeL: (Natural Language Processing Models Only Understand The Numerical Value. So We Need To Convert Textual Data To A Numerical Value.
#                          BOW is a simple model that converts sentences into bag of words with no meaning,
#                          here every word is assigned a unqiue number with the count of the number of ocuurance of the word.
#                          It focuses to represent the word not the order of the word)


from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
bow = bow_vectorizer.fit_transform(df['clean_tweet'])

bow[0]

bow[0].toarray()

# TfidVectorizer: (TF-IDF stands for Term Frequency Inverse Document Frequency of records.
#                  It can be defined as the calculation of how relevant a word in a series or corpus(data-set) is to a text. )

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
vector = vectorizer.fit_transform(df['clean_tweet'])

vector[0].toarray()

"""**Train-Test Split:**"""

# Spliting training and testing data

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow,df['label'],random_state=42,test_size=0.25)

print("Size of x_train:", (x_train.shape))
print("Size of y_train:", (y_train.shape))
print("Size of x_test:", (x_test.shape))
print("Size of y_test:", (y_test.shape))

"""**Model Training**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

"""**1. Logistic Regression:**"""

# training the model:
model = LogisticRegression()
model.fit(x_train,y_train)

# testing the model

pred = model.predict(x_test)
logi_acc = accuracy_score(y_test,pred)
print("Accuracy score: {:.2f}%".format(logi_acc*100))

# F1 score is a machine learning evaluation metric that measures a model's accuracy.
 # It combines the precision and recall scores of a model.
 # The accuracy metric computes how many times a model made a correct prediction across the entire dataset.
 # Precision measures how many of the “positive” predictions made by the model were correct.
 # Recall measures how many of the positive class samples present in the dataset were correctly identified by the model.

logi_f1 = f1_score(y_test,pred)
print("F1 Score: {:.2f}%".format(logi_f1*100))

# A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data.
# The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.

print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))

# confusion matrix from true values and predicted values
sns.heatmap(confusion_matrix(y_test,pred), annot=True)

# Hyperparameter tuning: (Hyperparameters are model parameters which value control the learning rate of the machine learning algorithm.
#                          By tuning the hyperparameters we can change the performance of machine learning model.
#                           There are various methods to perform hyperparater tuning, here we will use GridSearchCV)

from sklearn.model_selection import GridSearchCV

param_grid = {'C':[0.01,0.1,1,10]}
grid = GridSearchCV(LogisticRegression(),param_grid)
grid.fit(x_train,y_train)

print("Best Parameters:",grid.best_params_)

grid_pred = grid.predict(x_test)

logi_acc = accuracy_score(grid_pred,y_test)
print("Accuracy score: {:.2f}%".format(logi_acc*100))

logi_f1 = f1_score(y_test,grid_pred)
print("F1 Score: {:.2f}%".format(logi_f1*100))

print(confusion_matrix(y_test,grid_pred))
print('\n')
print(classification_report(y_test,grid_pred))

"""**2. Support Vector Classifier(SVC):**"""

from sklearn.svm import LinearSVC

# training the model:

SVCmodel = LinearSVC()
SVCmodel.fit(x_train,y_train)

# testing the model

svc_pred = SVCmodel.predict(x_test)
svc_acc = accuracy_score(svc_pred,y_test)
print("Accuracy score: {:.2f}%".format(svc_acc*100))

svc_f1 = f1_score(y_test,svc_pred)
print("F1 Score: {:.2f}%".format(svc_f1*100))

print(confusion_matrix(y_test,svc_pred))
print('\n')
print(classification_report(y_test,svc_pred))

# confusion matrix from true values and predicted values
sns.heatmap(confusion_matrix(y_test,svc_pred), annot=True)

grid = {
    'C':[0.01,0.1,1,10],
    'kernel':["linear","poly","rbf","sigmoid"],
    'degree':[1,3,5,7],
    'gamma':[0.01,1]
}
grid = GridSearchCV(SVCmodel,param_grid)
grid.fit(x_train,y_train)

print("Best Parameters:",grid.best_params_)

grid_pred = grid.predict(x_test)

svc_acc = accuracy_score(grid_pred,y_test)
print("Accuracy score: {:.2f}%".format(svc_acc*100))

svc_f1 = f1_score(y_test,grid_pred)
print("F1 Score: {:.2f}%".format(svc_f1*100))

print(confusion_matrix(y_test,grid_pred))
print('\n')
print(classification_report(y_test,grid_pred))

"""**3. Naive Bayes Classifier:**"""

from sklearn.naive_bayes import MultinomialNB

# training the model:

NBClassifier = MultinomialNB()
NBClassifier.fit(x_train,y_train)

# testing the model

NB_pred = NBClassifier.predict(x_test)
NB_acc = accuracy_score(NB_pred,y_test)
print("Accuracy score: {:.2f}%".format(NB_acc*100))

NB_f1 = f1_score(y_test,NB_pred)
print("F1 Score: {:.2f}%".format(NB_f1*100))

print(confusion_matrix(y_test,NB_pred))
print('\n')
print(classification_report(y_test,NB_pred))

# confusion matrix from true values and predicted values
sns.heatmap(confusion_matrix(y_test,NB_pred), annot=True)

"""4. **RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier

# training the model:

Ram_Classifier = RandomForestClassifier()
Ram_Classifier.fit(x_train,y_train)

# testing the model

Random_pred = Ram_Classifier.predict(x_test)
Random_acc = accuracy_score(Random_pred,y_test)
print("Accuracy score: {:.2f}%".format(Random_acc*100))

Ram_f1 = f1_score(y_test,Random_pred)
print("F1 Score: {:.2f}%".format(Ram_f1*100))

print(confusion_matrix(y_test,Random_pred))
print('\n')
print(classification_report(y_test,Random_pred))

# confusion matrix from true values and predicted values
sns.heatmap(confusion_matrix(y_test,Random_pred), annot=True)

"""5. **K-Nearest Neighbors Classifier**"""

from sklearn.neighbors import KNeighborsClassifier

# training the model:

K_Classifier = KNeighborsClassifier()
K_Classifier.fit(x_train,y_train)

# testing the model

K_pred = K_Classifier.predict(x_test)
K_acc = accuracy_score(K_pred,y_test)
print("Accuracy score: {:.2f}%".format(K_acc*100))

K_f1 = f1_score(y_test,K_pred)
print("F1 Score: {:.2f}%".format(K_f1*100))

print(confusion_matrix(y_test,K_pred))
print('\n')
print(classification_report(y_test,K_pred))

# confusion matrix from true values and predicted values
sns.heatmap(confusion_matrix(y_test,K_pred), annot=True)

